{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2bPtoanF9pe"
      },
      "source": [
        "# Finance Bill 2025 - RAG Chatbot\n",
        "**Tools:** LangChain · Groq (LLaMA 3.1) · ChromaDB · Sentence Transformers · Google Colab\n",
        "\n",
        "---\n",
        "\n",
        "## What is RAG?\n",
        "RAG stands for **Retrieval-Augmented Generation**. It combines two ideas:\n",
        "\n",
        "| Step | Name | What it does |\n",
        "|------|------|--------------|\n",
        "| 1 | **Retrieval** | Searches a document for the most relevant chunks based on your question |\n",
        "| 2 | **Augmented** | Combines those chunks into a structured context for the LLM |\n",
        "| 3 | **Generation** | The LLM reads the context and generates a precise, grounded answer |\n",
        "\n",
        "> **Why RAG instead of just asking an LLM?**  \n",
        "> LLMs are trained on general data and have a knowledge cutoff date.  \n",
        "> RAG lets them answer questions about *specific documents* (like the Finance Bill 2025)  \n",
        "> without hallucinating or guessing.\n",
        "\n",
        "---\n",
        "\n",
        "## Project Goal\n",
        "Build a chatbot that can answer questions about the **Kenya Finance Bill 2025**  \n",
        "using only the contents of the official PDF; with memory across follow-up questions."
      ],
      "id": "K2bPtoanF9pe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-eK8bDF9pg"
      },
      "source": [
        "## Step 1: Install Dependencies"
      ],
      "id": "_v-eK8bDF9pg"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LkVv8NOQF9ph"
      },
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "# - langchain_community: document loaders and vector store integrations\n",
        "# - langchain-groq: connects LangChain to Groq's LLaMA models\n",
        "# - pypdf: reads and parses PDF files\n",
        "# - chromadb: our vector database that stores embeddings\n",
        "# - sentence-transformers: converts text into numerical vectors (embeddings)\n",
        "\n",
        "!pip install numpy==1.26.4 -q\n",
        "!pip install langchain==0.3.0 langchain-community==0.3.0 langchain-groq pypdf chromadb sentence-transformers -q"
      ],
      "id": "LkVv8NOQF9ph"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30T9MuWfF9ph"
      },
      "source": [
        "## Step 2: Environment Setup"
      ],
      "id": "30T9MuWfF9ph"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dMwLvfypF9ph"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Suppress Chroma's telemetry warnings (this doesn't affect how the RAG works),\n",
        "# it just keeps our output clean\n",
        "os.environ[\"CHROMA_TELEMETRY\"] = \"False\""
      ],
      "id": "dMwLvfypF9ph"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATwopNZRF9pi"
      },
      "source": [
        "## Step 3: Mount Google Drive & Load API Key"
      ],
      "id": "ATwopNZRF9pi"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmEj1TsJF9pi",
        "outputId": "8bdec605-587e-454e-f2f5-f04143ac9ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive so we can access the Finance Bill PDF stored there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "bmEj1TsJF9pi"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVwIpi47F9pi",
        "outputId": "47b1687c-03ef-4ba5-aa96-cbadf7b942ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Load the Groq API key securely from Colab's Secrets\n",
        "# We use Colab Secrets instead of hardcoding the key to keep it safe\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('Groq_api_key')\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"API key not found. Add 'Groq_api_key' in Colab Secrets.\")\n",
        "\n",
        "print(\"API key loaded successfully.\")"
      ],
      "id": "wVwIpi47F9pi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SpYtP9CF9pi"
      },
      "source": [
        "## Step 4: Import Libraries"
      ],
      "id": "3SpYtP9CF9pi"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TrChJ3udF9pi"
      },
      "outputs": [],
      "source": [
        "# LangChain components\n",
        "from langchain_community.document_loaders import PyPDFLoader         # loads PDF pages as Document objects\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter  # splits text into overlapping chunks\n",
        "from langchain_community.vectorstores import Chroma                  # vector database which stores and searches embeddings\n",
        "from langchain.prompts import PromptTemplate                         # structures the instructions we send to the LLM\n",
        "from langchain.chains import LLMChain                                # links a prompt template to an LLM\n",
        "from langchain_core.messages import HumanMessage                     # formats messages for the LLM\n",
        "\n",
        "# Groq LLM\n",
        "from langchain_groq import ChatGroq  # connects to Groq's fast LLaMA inference engine\n",
        "\n",
        "# Embedding model\n",
        "from sentence_transformers import SentenceTransformer  # converts text to vectors that capture their meaning"
      ],
      "id": "TrChJ3udF9pi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXG3gyphF9pi"
      },
      "source": [
        "## Step 5: Load the Finance Bill PDF\n",
        "\n",
        "The PDF is loaded page by page. Each page becomes a *Document* object\n",
        "(think of it as a list where each item is one page of the bill)"
      ],
      "id": "OXG3gyphF9pi"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVhPmnytF9pi",
        "outputId": "fea9c07e-1f5f-4da1-d295-05702641db3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF loaded successfully. Total pages: 135\n"
          ]
        }
      ],
      "source": [
        "# Path to the Finance Bill PDF in Google Drive\n",
        "pdf_path = \"/content/drive/MyDrive/The Finance Bill 2025.pdf\"\n",
        "\n",
        "# PyPDFLoader reads the PDF and converts each page into a Document object\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"PDF loaded successfully. Total pages: {len(docs)}\")"
      ],
      "id": "nVhPmnytF9pi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1nq46FLF9pi"
      },
      "source": [
        "## Step 6: Split the Document into Chunks\n",
        "\n",
        "LLMs and vector databases cannot process an entire PDF at once.  \n",
        "We split the text into smaller, overlapping pieces called **chunks**.\n",
        "\n",
        "We use a **Parent-Child splitting strategy**:\n",
        "- **Child chunks** (smaller) → used for *searching*. Small = more precise matches\n",
        "- **Parent chunks** (larger) → used for *answering*. Large = more context for the LLM\n",
        "\n",
        "Think of it like a book index:\n",
        "- The index entry (child) points you to the right page\n",
        "- But you read the full paragraph (parent) to get the complete answer"
      ],
      "id": "V1nq46FLF9pi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parent splitter: larger chunks that give the LLM enough context to answer well\n",
        "parent_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,   # each parent chunk is up to 1000 characters\n",
        "    chunk_overlap=100  # 100 character overlap so we don't cut sentences between chunks\n",
        ")\n",
        "\n",
        "# Child splitter: smaller chunks used for precise similarity search\n",
        "child_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,    # smaller chunks = more focused search\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "# Split the PDF into parent chunks and count them\n",
        "parent_documents = parent_splitter.split_documents(docs)\n",
        "print(f\"Parent chunks created: {len(parent_documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfdJYWNTabwV",
        "outputId": "a0f9ce35-8741-4aa4-8ba9-8a0942bed6b7"
      },
      "id": "BfdJYWNTabwV",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parent chunks created: 356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atbJahTiF9pj"
      },
      "source": [
        "## Step 7: Set Up the Embedding Model\n",
        "\n",
        "An **embedding model** converts text into a list of numbers (a vector)  \n",
        "that captures the *meaning* of the text — not just the words.\n",
        "\n",
        "For example:\n",
        "- \"tax on imported goods\" and \"levy on foreign products\" would get *similar* vectors\n",
        "- even though they use different words\n",
        "\n",
        "This is what allows our retriever to find relevant chunks even when  \n",
        "the user's question is worded differently from the document text.\n",
        "\n",
        "We use `BAAI/bge-base-en` — a lightweight, high-quality open-source embedding model."
      ],
      "id": "atbJahTiF9pj"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "6b8ea2c5943649af915c5d9ba6607c10",
            "34e4b1879a5442ccac47e0c57f4112bd",
            "e03ce0d6f32c4ccf8101f7124d55a88d",
            "f20ed6d4245e4ababebbda8d61215d0c",
            "c86792bcfb84484ea137948d2267b27b",
            "3ec8f2868fea403c8019d16406d7e38e",
            "d9e382cd15a94d5fafef7efec42fa4fe",
            "63f9066a13924ada969a8c73a900d853",
            "f91c6912b3dc42fca5c5aba39ac2c409",
            "046f454255fc4a85a3559170eb21a412",
            "70d56022f238467d8b3848f0becb48c9"
          ]
        },
        "id": "kc0aODhZF9pj",
        "outputId": "9565a04c-9c32-4987-fd60-a1c529314356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b8ea2c5943649af915c5d9ba6607c10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: BAAI/bge-base-en\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model ready.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained embedding model\n",
        "bge_model = SentenceTransformer(\"BAAI/bge-base-en\")\n",
        "\n",
        "# Wrap it in a class so LangChain can use it\n",
        "# LangChain expects an object with embed_documents() and embed_query() methods\n",
        "class BGEEmbeddings:\n",
        "    def embed_documents(self, texts):\n",
        "        \"\"\"Embed a batch of document chunks (embeddings will be used when building the vector store)\"\"\"\n",
        "        return bge_model.encode(\n",
        "            texts,\n",
        "            batch_size=8,\n",
        "            normalize_embeddings=True  # normalizing improves search accuracy\n",
        "        ).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        \"\"\"Embed a single user question — used at query time\"\"\"\n",
        "        return bge_model.encode(\n",
        "            [text],\n",
        "            normalize_embeddings=True\n",
        "        ).tolist()[0]\n",
        "\n",
        "embedding_function = BGEEmbeddings()\n",
        "print(\"Embedding model ready.\")"
      ],
      "id": "kc0aODhZF9pj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOqjYHAYF9pj"
      },
      "source": [
        "## Step 8: Build the Vector Store and Retriever\n",
        "\n",
        "Now we embed all our chunks and store them in **ChromaDB** — our vector database.\n",
        "\n",
        "The **ParentDocumentRetriever** works like this:\n",
        "1. A user asks a question → it gets embedded into a vector\n",
        "2. ChromaDB finds the *child chunks* most similar to that vector\n",
        "3. The retriever then returns the corresponding *parent chunks* (with more context)\n",
        "4. Those parent chunks are passed to the LLM as context"
      ],
      "id": "AOqjYHAYF9pj"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEi3F0ijF9pj",
        "outputId": "6e603bd9-b922-4150-8a82-162f896642f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old vector store deleted.\n",
            "Vector store and retriever ready.\n"
          ]
        }
      ],
      "source": [
        "# Embed all parent chunks and store them in ChromaDB\n",
        "# Chroma.from_documents() takes our chunks, converts them to vectors using our\n",
        "# embedding function, and stores them in ChromaDB so we can search them later\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=parent_documents,    # the chunks we created from the PDF\n",
        "    embedding=embedding_function,  # the BGE model that converts text to vectors\n",
        "    persist_directory=\"./chroma_store\"  # saves to disk so we don't re-embed every run\n",
        ")\n",
        "\n",
        "# Build a simple retriever directly from the vector store\n",
        "# as_retriever() turns ChromaDB into a retriever object that LangChain can use\n",
        "# When a question comes in, it searches for the 5 most similar chunks to that question\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": 5}  # k=5 means return the top 5 most relevant chunks\n",
        ")\n",
        "\n",
        "print(\"Vector store and retriever ready.\")"
      ],
      "id": "xEi3F0ijF9pj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElFW5GNsF9pj"
      },
      "source": [
        "## Step 9: Set Up the LLM (Groq + LLaMA 3.1)\n",
        "\n",
        "The LLM is the **brain** of our RAG — it reads the retrieved context  \n",
        "and generates a natural language answer.\n",
        "\n",
        "We use **Groq's LLaMA 3.1** because:\n",
        "- It's fast (Groq's LPU hardware is purpose-built for inference)\n",
        "- It's free on the Groq API's generous free tier\n",
        "- LLaMA 3.1 is a strong open-source model for factual Q&A tasks\n",
        "\n",
        "`temperature=0.2` keeps answers factual and consistent —  \n",
        "closer to 0 means less creative/random, closer to 1 means more varied."
      ],
      "id": "ElFW5GNsF9pj"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egkva8LmF9pj",
        "outputId": "9920c639-239f-428c-8afa-06bfe91afb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM ready.\n"
          ]
        }
      ],
      "source": [
        "llm = ChatGroq(\n",
        "    api_key=api_key,\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.2  # low temperature = factual, consistent answers\n",
        ")\n",
        "\n",
        "print(\"LLM ready.\")"
      ],
      "id": "Egkva8LmF9pj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XDTb2MOF9pj"
      },
      "source": [
        "## Step 10: Debugging — Testing the LLM in Isolation\n",
        "\n",
        "Before building the full RAG pipeline, it's good practice to test each  \n",
        "component separately.\n",
        "\n",
        "Here we test whether the LLM itself can answer correctly  \n",
        "when given a manually written context — bypassing retrieval entirely.\n",
        "\n",
        "> **Finding:** The LLM answered correctly when given clear context.  \n",
        "> This told us the problem was in *retrieval*, not in the LLM."
      ],
      "id": "8XDTb2MOF9pj"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvtSvmiOF9pj",
        "outputId": "31963f1c-da0a-4702-e100-25dee794f1a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Test Answer:\n",
            "According to the context, the new environmental tax introduced in the Finance Bill 2025 is a 10% environmental levy on plastic packaging materials.\n"
          ]
        }
      ],
      "source": [
        "# Manually provide context to test the LLM directly\n",
        "test_context = \"\"\"\n",
        "The Finance Bill 2025 introduces a 5% digital service tax on revenue earned\n",
        "by non-resident persons providing digital services in Kenya.\n",
        "It also proposes a 10% environmental levy on plastic packaging materials.\n",
        "There will be a 3% luxury tax on imported vehicles valued above KES 5 million.\n",
        "\"\"\"\n",
        "\n",
        "test_question = \"What new environmental taxes are introduced in the bill?\"\n",
        "\n",
        "test_prompt = f\"\"\"Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{test_context}\n",
        "\n",
        "Question: {test_question}\n",
        "\"\"\"\n",
        "\n",
        "response = llm.invoke([HumanMessage(content=test_prompt)])\n",
        "print(\"LLM Test Answer:\")\n",
        "print(response.content)"
      ],
      "id": "VvtSvmiOF9pj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9r05RnwF9pk"
      },
      "source": [
        "## Step 11: Build the Final RAG Function with Memory\n",
        "\n",
        "The default LangChain `RetrievalQA` chain passes context to the LLM in a way  \n",
        "that doesn't always work well for specific structured questions.\n",
        "\n",
        "We fix this by:\n",
        "1. **Custom prompt template** — we explicitly tell the LLM to use only the provided context\n",
        "2. **Custom chain** — we control exactly what gets passed to the LLM\n",
        "3. **Memory** — we store the conversation history so the LLM can handle follow-up questions\n",
        "\n",
        "> **Memory analogy:** Without memory, each question is like calling a new person.  \n",
        "> With memory, it's like continuing a conversation with someone who remembers  \n",
        "> everything you've discussed."
      ],
      "id": "P9r05RnwF9pk"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VP-liSbF9pk",
        "outputId": "6c5e91bf-ccbd-4d07-dcae-1612e52aa54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG function ready. You can now ask questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3822896775.py:26: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  custom_chain = LLMChain(llm=llm, prompt=custom_prompt)\n"
          ]
        }
      ],
      "source": [
        "# Initialise chat history (this list grows as the conversation progresses)\n",
        "chat_history = []\n",
        "\n",
        "# Step 1: Define a custom prompt template\n",
        "# {context} = the retrieved chunks from the PDF\n",
        "# {question} = the user's question\n",
        "# {chat_history} = previous Q&A pairs so the LLM can handle follow-ups\n",
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\", \"chat_history\"],\n",
        "    template=\"\"\"You are a helpful assistant that answers questions about the Kenya Finance Bill 2025.\n",
        "Use ONLY the context provided below to answer the question.\n",
        "If the answer is not in the context, say \"I could not find that information in the Finance Bill 2025.\"\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Context from the Finance Bill:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        ")\n",
        "\n",
        "# Step 2: Create the LLM chain — links the prompt template to the LLM\n",
        "custom_chain = LLMChain(llm=llm, prompt=custom_prompt)\n",
        "\n",
        "# Step 3: Define the main RAG function\n",
        "def rag_with_memory(question):\n",
        "    \"\"\"\n",
        "    Ask a question about the Finance Bill 2025.\n",
        "    The function retrieves relevant context from the PDF,\n",
        "    builds a prompt with conversation history, and returns the LLM's answer.\n",
        "    \"\"\"\n",
        "    # Retrieve the top 5 most relevant chunks from the vector store\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Combine the retrieved chunks into one context string\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Format the conversation history for the prompt\n",
        "    past_conversation = \"\\n\".join(chat_history) if chat_history else \"No previous conversation.\"\n",
        "\n",
        "    # Run the LLM chain with the context, question, and history\n",
        "    response = custom_chain.invoke({\n",
        "        \"context\": context,\n",
        "        \"question\": question,\n",
        "        \"chat_history\": past_conversation\n",
        "    })\n",
        "\n",
        "    answer = response[\"text\"]\n",
        "\n",
        "    # Save this Q&A to memory for future follow-up questions\n",
        "    chat_history.append(f\"User: {question}\")\n",
        "    chat_history.append(f\"Assistant: {answer}\")\n",
        "\n",
        "    return answer\n",
        "\n",
        "print(\"RAG function ready. You can now ask questions.\")"
      ],
      "id": "_VP-liSbF9pk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KehqS08oF9pk"
      },
      "source": [
        "## Step 12: Ask Questions\n",
        "\n",
        "Now we can query the Finance Bill 2025 in natural language.  \n",
        "The chatbot will remember previous questions in the same session."
      ],
      "id": "KehqS08oF9pk"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHu-y9NbF9pk",
        "outputId": "e5e0f297-e684-4eaa-e8cb-f7cb2bd5e59b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3822896775.py:36: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = retriever.get_relevant_documents(question)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: The digital service tax is not explicitly defined in the provided context. However, it is mentioned in Section 42B of the Tax Procedures Act (Cap. 469B) that the Commissioner may appoint an agent for the purpose of collection and remittance of digital service tax to the Commissioner. \n",
            "\n",
            "However, the rate of tax in respect of digital service tax is mentioned in Section 12 of the Finance Bill 2025, which states that the rate of tax in respect of digital service tax shall be five percent of the gross amount.\n"
          ]
        }
      ],
      "source": [
        "# Question 1\n",
        "answer1 = rag_with_memory(\"What is the digital service tax introduced in the Finance Bill 2025?\")\n",
        "print(\"Q1:\", answer1)"
      ],
      "id": "GHu-y9NbF9pk"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1ouXYTVF9pk",
        "outputId": "65e3eaf2-3864-420e-9ab0-63e566053934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q2: According to Section 2 of the provided context, an agent is required to pay the amount specified in the notice to the Commissioner. An agent is defined as a person who owes or may subsequently owe money to the taxpayer, or who holds or may subsequently hold money for or on account of the taxpayer.\n"
          ]
        }
      ],
      "source": [
        "# Question 2, follow-up. The RAG remembers Q1 so \"it\" refers to the digital service tax\n",
        "answer2 = rag_with_memory(\"Who is required to pay it?\")\n",
        "print(\"Q2:\", answer2)"
      ],
      "id": "-1ouXYTVF9pk"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTUYtpMTF9pk",
        "outputId": "2aef1275-cd85-431b-8b34-44145d8b695d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3: I could not find that information in the Finance Bill 2025.\n"
          ]
        }
      ],
      "source": [
        "# Question 3\n",
        "answer3 = rag_with_memory(\"What new environmental taxes are introduced in the bill?\")\n",
        "print(\"Q3:\", answer3)"
      ],
      "id": "WTUYtpMTF9pk"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B4FD4l7F9pk",
        "outputId": "4f931dfb-d6a2-4c97-d28a-df1afb5b60e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q4: Based on the provided context, I could not find a comprehensive list of new taxes introduced in the Finance Bill 2025. However, I can identify a few new taxes or amendments related to taxes:\n",
            "\n",
            "1. Digital service tax: The rate of tax in respect of digital service tax is mentioned in Section 12 of the Finance Bill 2025, which states that the rate of tax in respect of digital service tax shall be five percent of the gross amount.\n",
            "\n",
            "2. Electronic tax invoices: The Finance Bill 2025 proposes to amend the Tax Procedures Act to require a person who carries on business to issue an electronic tax invoice through a system established by the Commissioner.\n",
            "\n",
            "3. Amendments to the Income Tax Act: The Finance Bill 2025 proposes various amendments to the Income Tax Act, including changes to the definitions of \"gross investment receipts\" and the insertion of new subsections related to tax deductions and assessments.\n",
            "\n",
            "However, I could not find a comprehensive list of new taxes introduced in the Finance Bill 2025.\n"
          ]
        }
      ],
      "source": [
        "# Question 4\n",
        "answer4 = rag_with_memory(\"List at least 3 new taxes introduced in the Finance Bill 2025.\")\n",
        "print(\"Q4:\", answer4)"
      ],
      "id": "6B4FD4l7F9pk"
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_with_memory(\"What is the purpose of the Finance Bill 2025?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfREC1B-kb8O",
        "outputId": "dfd581ca-52d4-4e97-b405-501299224aca"
      },
      "id": "ZfREC1B-kb8O",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The purpose of the Finance Bill 2025 is to formulate proposals relating to revenue raising measures including liability to, and collection of taxes. It proposes to amend various laws relating to taxes and duties.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b8ea2c5943649af915c5d9ba6607c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34e4b1879a5442ccac47e0c57f4112bd",
              "IPY_MODEL_e03ce0d6f32c4ccf8101f7124d55a88d",
              "IPY_MODEL_f20ed6d4245e4ababebbda8d61215d0c"
            ],
            "layout": "IPY_MODEL_c86792bcfb84484ea137948d2267b27b"
          }
        },
        "34e4b1879a5442ccac47e0c57f4112bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ec8f2868fea403c8019d16406d7e38e",
            "placeholder": "​",
            "style": "IPY_MODEL_d9e382cd15a94d5fafef7efec42fa4fe",
            "value": "Loading weights: 100%"
          }
        },
        "e03ce0d6f32c4ccf8101f7124d55a88d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63f9066a13924ada969a8c73a900d853",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f91c6912b3dc42fca5c5aba39ac2c409",
            "value": 199
          }
        },
        "f20ed6d4245e4ababebbda8d61215d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_046f454255fc4a85a3559170eb21a412",
            "placeholder": "​",
            "style": "IPY_MODEL_70d56022f238467d8b3848f0becb48c9",
            "value": " 199/199 [00:00&lt;00:00, 531.56it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "c86792bcfb84484ea137948d2267b27b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec8f2868fea403c8019d16406d7e38e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9e382cd15a94d5fafef7efec42fa4fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63f9066a13924ada969a8c73a900d853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91c6912b3dc42fca5c5aba39ac2c409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "046f454255fc4a85a3559170eb21a412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70d56022f238467d8b3848f0becb48c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}